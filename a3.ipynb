{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Core: Exploring and understanding the Data\r\n",
    "\r\n",
    "All imports are sorted and copy from the imports that I use in assignment2."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "# importing pandas, and other necessary modules\r\n",
    "import io\r\n",
    "import sys\r\n",
    "\r\n",
    "import matplotlib.image as mpimg\r\n",
    "import matplotlib.pyplot as plt  # visualising\r\n",
    "import missingno as msno\r\n",
    "import numpy as np  # linear algebra\r\n",
    "# import pandas, and other necessary modules\r\n",
    "import pandas as pd  # data processing\r\n",
    "# easy for structing the report , need pip install pandas-profiling first\r\n",
    "import pandas_profiling as pp\r\n",
    "import phik\r\n",
    "import pydotplus\r\n",
    "import seaborn as sns  # visualising\r\n",
    "from IPython.display import Image\r\n",
    "from numpy import cov\r\n",
    "from scipy.stats import pearsonr\r\n",
    "from sklearn import datasets, tree\r\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\r\n",
    "from sklearn.feature_selection import SelectKBest, chi2\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.pipeline import make_pipeline\r\n",
    "from sklearn.preprocessing import (KBinsDiscretizer, LabelEncoder,\r\n",
    "                                   OneHotEncoder, OrdinalEncoder,\r\n",
    "                                   StandardScaler)\r\n",
    "from sklearn.svm import SVR\r\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\r\n",
    "\r\n",
    "import glob\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "load dataset and merged them into 1 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "alternative = pd.read_csv('datasets/alternative.csv')\r\n",
    "anime = pd.read_csv(\"datasets/anime.csv\")\r\n",
    "blues = pd.read_csv('datasets/blues.csv')\r\n",
    "classical = pd.read_csv('datasets/classical.csv')\r\n",
    "country = pd.read_csv('datasets/country.csv')\r\n",
    "electronics = pd.read_csv('datasets/electronic.csv')\r\n",
    "hip_hop = pd.read_csv('datasets/hip-hop.csv')\r\n",
    "jazz = pd.read_csv('datasets/jazz.csv')\r\n",
    "rap = pd.read_csv('datasets/rap.csv')\r\n",
    "rock = pd.read_csv('datasets/rock.csv')\r\n",
    "test_instances = pd.read_csv('datasets/testing-instances.csv')\r\n",
    "\r\n",
    "dataset_list = [\r\n",
    "    alternative,\r\n",
    "    anime,\r\n",
    "    blues,\r\n",
    "    classical,\r\n",
    "    country,\r\n",
    "    electronics,\r\n",
    "    hip_hop,\r\n",
    "    jazz,\r\n",
    "    rap,\r\n",
    "    rock\r\n",
    "]\r\n",
    "\r\n",
    "# dataset_name_list = [\"alternative\", \"anime\", \"blues\", \"classical\",\r\n",
    "#                      \"country\", \"electronics\", \"hip_hop\", \"jazz\", \"rap\", \"rock\"]\r\n",
    "# name_dataset_map = dict(zip(dataset_name_list, dataset_list))\r\n",
    "\r\n",
    "# set the pd to display every rows and cols, so no folding\r\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\r\n",
    "\r\n",
    "\r\n",
    "def merge_multi_datasets(data_list=dataset_list):\r\n",
    "    '''merge multiple dataset into one and then return it.\r\n",
    "    For example, in data_list, it hold rock,rap ... etc, this method will merge it  using outer\r\n",
    "    '''\r\n",
    "    merged_all = pd.DataFrame()\r\n",
    "    # print(merged_all)\r\n",
    "    for dataset in data_list:\r\n",
    "        # In order to merge the data without errors, we need to align the index of the \"Country/Area\" column to keep consistency between data frames. We reset the index value from 0 by sorting the \"Country/Area\" column in each dataset\r\n",
    "        dataset.sort_values(by=['instance_id'], inplace=True)\r\n",
    "\r\n",
    "        # reset index values so dataframes can be merged without errors\r\n",
    "        data_to_be_merged = dataset.reset_index(drop=True)\r\n",
    "\r\n",
    "        # 4. finially, merged datasets.\r\n",
    "        if merged_all.empty:\r\n",
    "            merged_all = data_to_be_merged\r\n",
    "        else:\r\n",
    "            merged_all = pd.merge(merged_all, data_to_be_merged, how='outer')\r\n",
    "\r\n",
    "    return merged_all\r\n",
    "# type(rock)\r\n",
    "\r\n",
    "\r\n",
    "# check the shapes and if there is any missing values\r\n",
    "def print_NA_details(dataset):\r\n",
    "    '''method for checking whether the passed in dataset has missing (i.e. NA) fetatures '''\r\n",
    "    missing = dataset.isnull().sum()\r\n",
    "    missing = missing[missing > 0]\r\n",
    "    print('\\nmissing feature and values:\\n', missing)\r\n",
    "\r\n",
    "\r\n",
    "merged_dataset = merge_multi_datasets()\r\n",
    "merged_dataset.to_csv(\"datasets/merged_all.csv\", encoding='utf-8', index=False)\r\n",
    "\r\n",
    "print(merged_dataset.info())\r\n",
    "print(merged_dataset.shape)\r\n",
    "print_NA_details(merged_dataset)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 50000 entries, 0 to 49999\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   instance_id       50000 non-null  int64  \n",
      " 1   artist_name       50000 non-null  object \n",
      " 2   track_hash        50000 non-null  object \n",
      " 3   track_name        50000 non-null  object \n",
      " 4   popularity        50000 non-null  int64  \n",
      " 5   acousticness      50000 non-null  float64\n",
      " 6   danceability      50000 non-null  float64\n",
      " 7   duration_ms       50000 non-null  int64  \n",
      " 8   energy            50000 non-null  float64\n",
      " 9   instrumentalness  50000 non-null  float64\n",
      " 10  key               50000 non-null  object \n",
      " 11  liveness          50000 non-null  float64\n",
      " 12  loudness          50000 non-null  float64\n",
      " 13  mode              50000 non-null  object \n",
      " 14  speechiness       50000 non-null  float64\n",
      " 15  tempo             50000 non-null  object \n",
      " 16  obtained_date     50000 non-null  object \n",
      " 17  valence           50000 non-null  float64\n",
      " 18  music_genre       50000 non-null  object \n",
      "dtypes: float64(8), int64(3), object(8)\n",
      "memory usage: 7.6+ MB\n",
      "None\n",
      "(50000, 19)\n",
      "\n",
      "missing feature and values:\n",
      " Series([], dtype: int64)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}