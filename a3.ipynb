{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Core: Exploring and understanding the Data\r\n",
    "\r\n",
    "All imports are sorted and copy from the imports that I use in assignment2."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# importing pandas, and other necessary modules\r\n",
    "import io\r\n",
    "import sys\r\n",
    "\r\n",
    "import matplotlib.image as mpimg\r\n",
    "import matplotlib.pyplot as plt  # visualising\r\n",
    "import missingno as msno\r\n",
    "import numpy as np  # linear algebra\r\n",
    "# import pandas, and other necessary modules\r\n",
    "import pandas as pd  # data processing\r\n",
    "# easy for structing the report , need pip install pandas-profiling first\r\n",
    "import pandas_profiling as pp\r\n",
    "import phik\r\n",
    "import pydotplus\r\n",
    "import seaborn as sns  # visualising\r\n",
    "from IPython.display import Image\r\n",
    "from numpy import cov\r\n",
    "from scipy.stats import pearsonr\r\n",
    "from sklearn import datasets, tree\r\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\r\n",
    "from sklearn.feature_selection import SelectKBest, chi2\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.pipeline import make_pipeline\r\n",
    "from sklearn.preprocessing import (KBinsDiscretizer, LabelEncoder,\r\n",
    "                                   OneHotEncoder, OrdinalEncoder,\r\n",
    "                                   StandardScaler)\r\n",
    "from sklearn.svm import SVR\r\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\r\n",
    "\r\n",
    "import glob\r\n",
    "import sklearn\r\n",
    "\r\n",
    "import myutil #import my tool script\r\n",
    "\r\n",
    "# Encoding categorical features with preserving the missing values in incomplete features\r\n",
    "from sklearn.preprocessing import OrdinalEncoder\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. load dataset and merged them into 1 whole dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "alternative = pd.read_csv('datasets/alternative.csv')\r\n",
    "anime = pd.read_csv(\"datasets/anime.csv\")\r\n",
    "blues = pd.read_csv('datasets/blues.csv')\r\n",
    "classical = pd.read_csv('datasets/classical.csv')\r\n",
    "country = pd.read_csv('datasets/country.csv')\r\n",
    "electronics = pd.read_csv('datasets/electronic.csv')\r\n",
    "hip_hop = pd.read_csv('datasets/hip-hop.csv')\r\n",
    "jazz = pd.read_csv('datasets/jazz.csv')\r\n",
    "rap = pd.read_csv('datasets/rap.csv')\r\n",
    "rock = pd.read_csv('datasets/rock.csv')\r\n",
    "test_instances = pd.read_csv('datasets/testing-instances.csv')\r\n",
    "\r\n",
    "dataset_list = [\r\n",
    "    alternative,\r\n",
    "    anime,\r\n",
    "    blues,\r\n",
    "    classical,\r\n",
    "    country,\r\n",
    "    electronics,\r\n",
    "    hip_hop,\r\n",
    "    jazz,\r\n",
    "    rap,\r\n",
    "    rock\r\n",
    "]\r\n",
    "\r\n",
    "# dataset_name_list = [\"alternative\", \"anime\", \"blues\", \"classical\",\r\n",
    "#                      \"country\", \"electronics\", \"hip_hop\", \"jazz\", \"rap\", \"rock\"]\r\n",
    "# name_dataset_map = dict(zip(dataset_name_list, dataset_list))\r\n",
    "\r\n",
    "# set the pd to display every rows and cols, so no folding\r\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\r\n",
    "\r\n",
    "\r\n",
    "def merge_multi_datasets(data_list=dataset_list):\r\n",
    "    '''merge multiple dataset into one and then return it.\r\n",
    "    For example, in data_list, it hold rock,rap ... etc, this method will merge it  using outer\r\n",
    "    '''\r\n",
    "    merged_all = pd.DataFrame()\r\n",
    "    # print(merged_all)\r\n",
    "    for dataset in data_list:\r\n",
    "        # In order to merge the data without errors, we need to align the index of the \"Country/Area\" column to keep consistency between data frames. We reset the index value from 0 by sorting the \"Country/Area\" column in each dataset\r\n",
    "        dataset.sort_values(by=['instance_id'], inplace=True)\r\n",
    "\r\n",
    "        # reset index values so dataframes can be merged without errors\r\n",
    "        data_to_be_merged = dataset.reset_index(drop=True)\r\n",
    "\r\n",
    "        # 4. finially, merged datasets.\r\n",
    "        if merged_all.empty:\r\n",
    "            merged_all = data_to_be_merged\r\n",
    "        else:\r\n",
    "            merged_all = pd.merge(merged_all, data_to_be_merged, how='outer')\r\n",
    "\r\n",
    "    return merged_all\r\n",
    "# type(rock)\r\n",
    "\r\n",
    "\r\n",
    "# check the shapes and if there is any missing values\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "merged_dataset = merge_multi_datasets()\r\n",
    "\r\n",
    "print(merged_dataset.columns)\r\n",
    "print(merged_dataset.shape)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['instance_id', 'artist_name', 'track_hash', 'track_name', 'popularity',\n",
      "       'acousticness', 'danceability', 'duration_ms', 'energy',\n",
      "       'instrumentalness', 'key', 'liveness', 'loudness', 'mode',\n",
      "       'speechiness', 'tempo', 'obtained_date', 'valence', 'music_genre'],\n",
      "      dtype='object')\n",
      "(50000, 19)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Clean the potential dirty data.\r\n",
    "\r\n",
    "such as duplicated, missing, etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "check whether there are conspicuous NA, which is any missing features among all rows."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(merged_dataset.info())\r\n",
    "myutil.print_NA_details(merged_dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 50000 entries, 0 to 49999\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   instance_id       50000 non-null  int64  \n",
      " 1   artist_name       50000 non-null  object \n",
      " 2   track_hash        50000 non-null  object \n",
      " 3   track_name        50000 non-null  object \n",
      " 4   popularity        50000 non-null  int64  \n",
      " 5   acousticness      50000 non-null  float64\n",
      " 6   danceability      50000 non-null  float64\n",
      " 7   duration_ms       50000 non-null  int64  \n",
      " 8   energy            50000 non-null  float64\n",
      " 9   instrumentalness  50000 non-null  float64\n",
      " 10  key               50000 non-null  object \n",
      " 11  liveness          50000 non-null  float64\n",
      " 12  loudness          50000 non-null  float64\n",
      " 13  mode              50000 non-null  object \n",
      " 14  speechiness       50000 non-null  float64\n",
      " 15  tempo             50000 non-null  object \n",
      " 16  obtained_date     50000 non-null  object \n",
      " 17  valence           50000 non-null  float64\n",
      " 18  music_genre       50000 non-null  object \n",
      "dtypes: float64(8), int64(3), object(8)\n",
      "memory usage: 7.6+ MB\n",
      "None\n",
      "\n",
      "missing feature and values:\n",
      " Series([], dtype: int64)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "No conspicuous missing. \r\n",
    "\r\n",
    "Then, check if there are any duplicates based on `track_hash` and `track_name`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# merged_dataset.tail()\r\n",
    "duplicateDFRow = merged_dataset[merged_dataset.duplicated(['track_hash','track_name'])]\r\n",
    "print(duplicateDFRow)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Empty DataFrame\n",
      "Columns: [instance_id, artist_name, track_hash, track_name, popularity, acousticness, danceability, duration_ms, energy, instrumentalness, key, liveness, loudness, mode, speechiness, tempo, obtained_date, valence, music_genre]\n",
      "Index: []\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From above, we know that there are no conspicuous missing and duplicates.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "generate the `panda profile report and export it as html`\r\n",
    "\r\n",
    "Not display to widget is because I am using vs code and when quit and reopen again, it will disappear. Therefore, read it via static html is more friendly to myself."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "profile_report = pp.ProfileReport(merged_dataset)\r\n",
    "# export to html, since look at the static HTML is more friendly to me  \r\n",
    "profile_report.to_file('panda-profile-report/before_merged_dataset.html')\r\n",
    "merged_dataset.head()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Summarize dataset: 100%|██████████| 32/32 [00:29<00:00,  1.09it/s, Completed]\n",
      "Generate report structure: 100%|██████████| 1/1 [00:08<00:00,  8.48s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:04<00:00,  4.58s/it]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 30.68it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_hash</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>obtained_date</th>\n",
       "      <th>valence</th>\n",
       "      <th>music_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20012</td>\n",
       "      <td>Avenged Sevenfold</td>\n",
       "      <td>0caajoOsHzQOZtIXitnRUN</td>\n",
       "      <td>Requiem</td>\n",
       "      <td>49</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.532</td>\n",
       "      <td>261573</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.007650</td>\n",
       "      <td>G#</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-5.667</td>\n",
       "      <td>Major</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>142.016</td>\n",
       "      <td>4-Apr</td>\n",
       "      <td>0.485</td>\n",
       "      <td>Alternative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20018</td>\n",
       "      <td>BADBADNOTGOOD</td>\n",
       "      <td>0bdabO15YOj0iZPg2OujAw</td>\n",
       "      <td>In Your Eyes (feat. Charlotte Day Wilson)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.495</td>\n",
       "      <td>247055</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>C</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-5.802</td>\n",
       "      <td>Minor</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>79.71</td>\n",
       "      <td>4-Apr</td>\n",
       "      <td>0.424</td>\n",
       "      <td>Alternative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20042</td>\n",
       "      <td>System Of A Down</td>\n",
       "      <td>0blIe8ZSUusQfh4hvBNWoD</td>\n",
       "      <td>F**k The System</td>\n",
       "      <td>48</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.474</td>\n",
       "      <td>132733</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>G#</td>\n",
       "      <td>0.314</td>\n",
       "      <td>-1.884</td>\n",
       "      <td>Major</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>171.433</td>\n",
       "      <td>4-Apr</td>\n",
       "      <td>0.847</td>\n",
       "      <td>Alternative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20057</td>\n",
       "      <td>Jeremy Camp</td>\n",
       "      <td>0CBM2iiBZmfKntDeQYboqU</td>\n",
       "      <td>There Will Be A Day</td>\n",
       "      <td>48</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.344</td>\n",
       "      <td>279440</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>F#</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-5.705</td>\n",
       "      <td>Major</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>77.291</td>\n",
       "      <td>4-Apr</td>\n",
       "      <td>0.208</td>\n",
       "      <td>Alternative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20066</td>\n",
       "      <td>Three Days Grace</td>\n",
       "      <td>0c1gHntWjKD7QShC8s99sq</td>\n",
       "      <td>It's All Over</td>\n",
       "      <td>50</td>\n",
       "      <td>0.003050</td>\n",
       "      <td>0.321</td>\n",
       "      <td>249320</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>C</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-4.459</td>\n",
       "      <td>Minor</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>?</td>\n",
       "      <td>4-Apr</td>\n",
       "      <td>0.198</td>\n",
       "      <td>Alternative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instance_id        artist_name              track_hash  \\\n",
       "0        20012  Avenged Sevenfold  0caajoOsHzQOZtIXitnRUN   \n",
       "1        20018      BADBADNOTGOOD  0bdabO15YOj0iZPg2OujAw   \n",
       "2        20042   System Of A Down  0blIe8ZSUusQfh4hvBNWoD   \n",
       "3        20057        Jeremy Camp  0CBM2iiBZmfKntDeQYboqU   \n",
       "4        20066   Three Days Grace  0c1gHntWjKD7QShC8s99sq   \n",
       "\n",
       "                                  track_name  popularity  acousticness  \\\n",
       "0                                    Requiem          49      0.000104   \n",
       "1  In Your Eyes (feat. Charlotte Day Wilson)          50      0.430000   \n",
       "2                            F**k The System          48      0.002160   \n",
       "3                        There Will Be A Day          48      0.109000   \n",
       "4                              It's All Over          50      0.003050   \n",
       "\n",
       "   danceability  duration_ms  energy  instrumentalness key  liveness  \\\n",
       "0         0.532       261573   0.953          0.007650  G#     0.119   \n",
       "1         0.495       247055   0.533          0.000672   C     0.106   \n",
       "2         0.474       132733   0.994          0.126000  G#     0.314   \n",
       "3         0.344       279440   0.667          0.000000  F#     0.103   \n",
       "4         0.321       249320   0.820          0.000000   C     0.340   \n",
       "\n",
       "   loudness   mode  speechiness    tempo obtained_date  valence  music_genre  \n",
       "0    -5.667  Major       0.0394  142.016         4-Apr    0.485  Alternative  \n",
       "1    -5.802  Minor       0.0295    79.71         4-Apr    0.424  Alternative  \n",
       "2    -1.884  Major       0.1030  171.433         4-Apr    0.847  Alternative  \n",
       "3    -5.705  Major       0.0385   77.291         4-Apr    0.208  Alternative  \n",
       "4    -4.459  Minor       0.0422        ?         4-Apr    0.198  Alternative  "
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**read csv again since we need to specify which values are missing and/or unusual.** \r\n",
    "\r\n",
    "It is because there are potential missing values.\r\n",
    "For instances, in `tempo`, there are `?`. ANd for 'duration_ms', there are `-1`,which are missing values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "merged_dataset.to_csv(\"datasets/merged_all.csv\", encoding='utf-8', index=False)\r\n",
    "merged_dataset = pd.read_csv(\"datasets/merged_all.csv\",na_values=['?','NA','missing','not available','-','-1','empty_field'])\r\n",
    "myutil.print_NA_details(merged_dataset)\r\n",
    "merged_dataset.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "missing feature and values:\n",
      " artist_name    2489\n",
      "duration_ms    4939\n",
      "tempo          4980\n",
      "dtype: int64\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_hash</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>obtained_date</th>\n",
       "      <th>valence</th>\n",
       "      <th>music_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20012</td>\n",
       "      <td>Avenged Sevenfold</td>\n",
       "      <td>0caajoOsHzQOZtIXitnRUN</td>\n",
       "      <td>Requiem</td>\n",
       "      <td>49</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.532</td>\n",
       "      <td>261573.0</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.007650</td>\n",
       "      <td>G#</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-5.667</td>\n",
       "      <td>Major</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>142.016</td>\n",
       "      <td>4-Apr</td>\n",
       "      <td>0.485</td>\n",
       "      <td>Alternative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20018</td>\n",
       "      <td>BADBADNOTGOOD</td>\n",
       "      <td>0bdabO15YOj0iZPg2OujAw</td>\n",
       "      <td>In Your Eyes (feat. Charlotte Day Wilson)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.495</td>\n",
       "      <td>247055.0</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>C</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-5.802</td>\n",
       "      <td>Minor</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>79.710</td>\n",
       "      <td>4-Apr</td>\n",
       "      <td>0.424</td>\n",
       "      <td>Alternative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20042</td>\n",
       "      <td>System Of A Down</td>\n",
       "      <td>0blIe8ZSUusQfh4hvBNWoD</td>\n",
       "      <td>F**k The System</td>\n",
       "      <td>48</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.474</td>\n",
       "      <td>132733.0</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>G#</td>\n",
       "      <td>0.314</td>\n",
       "      <td>-1.884</td>\n",
       "      <td>Major</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>171.433</td>\n",
       "      <td>4-Apr</td>\n",
       "      <td>0.847</td>\n",
       "      <td>Alternative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20057</td>\n",
       "      <td>Jeremy Camp</td>\n",
       "      <td>0CBM2iiBZmfKntDeQYboqU</td>\n",
       "      <td>There Will Be A Day</td>\n",
       "      <td>48</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.344</td>\n",
       "      <td>279440.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>F#</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-5.705</td>\n",
       "      <td>Major</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>77.291</td>\n",
       "      <td>4-Apr</td>\n",
       "      <td>0.208</td>\n",
       "      <td>Alternative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20066</td>\n",
       "      <td>Three Days Grace</td>\n",
       "      <td>0c1gHntWjKD7QShC8s99sq</td>\n",
       "      <td>It's All Over</td>\n",
       "      <td>50</td>\n",
       "      <td>0.003050</td>\n",
       "      <td>0.321</td>\n",
       "      <td>249320.0</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>C</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-4.459</td>\n",
       "      <td>Minor</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4-Apr</td>\n",
       "      <td>0.198</td>\n",
       "      <td>Alternative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instance_id        artist_name              track_hash  \\\n",
       "0        20012  Avenged Sevenfold  0caajoOsHzQOZtIXitnRUN   \n",
       "1        20018      BADBADNOTGOOD  0bdabO15YOj0iZPg2OujAw   \n",
       "2        20042   System Of A Down  0blIe8ZSUusQfh4hvBNWoD   \n",
       "3        20057        Jeremy Camp  0CBM2iiBZmfKntDeQYboqU   \n",
       "4        20066   Three Days Grace  0c1gHntWjKD7QShC8s99sq   \n",
       "\n",
       "                                  track_name  popularity  acousticness  \\\n",
       "0                                    Requiem          49      0.000104   \n",
       "1  In Your Eyes (feat. Charlotte Day Wilson)          50      0.430000   \n",
       "2                            F**k The System          48      0.002160   \n",
       "3                        There Will Be A Day          48      0.109000   \n",
       "4                              It's All Over          50      0.003050   \n",
       "\n",
       "   danceability  duration_ms  energy  instrumentalness key  liveness  \\\n",
       "0         0.532     261573.0   0.953          0.007650  G#     0.119   \n",
       "1         0.495     247055.0   0.533          0.000672   C     0.106   \n",
       "2         0.474     132733.0   0.994          0.126000  G#     0.314   \n",
       "3         0.344     279440.0   0.667          0.000000  F#     0.103   \n",
       "4         0.321     249320.0   0.820          0.000000   C     0.340   \n",
       "\n",
       "   loudness   mode  speechiness    tempo obtained_date  valence  music_genre  \n",
       "0    -5.667  Major       0.0394  142.016         4-Apr    0.485  Alternative  \n",
       "1    -5.802  Minor       0.0295   79.710         4-Apr    0.424  Alternative  \n",
       "2    -1.884  Major       0.1030  171.433         4-Apr    0.847  Alternative  \n",
       "3    -5.705  Major       0.0385   77.291         4-Apr    0.208  Alternative  \n",
       "4    -4.459  Minor       0.0422      NaN         4-Apr    0.198  Alternative  "
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## delete instance_id field, since this is 100% unnecessuary feature.\r\n",
    "> https://stackoverflow.com/questions/13411544/delete-a-column-from-a-pandas-dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# the index of instance_id is 0\r\n",
    "merged_dataset.drop(merged_dataset.columns[[0]], axis=1, inplace=True)\r\n",
    "merged_dataset.tail()  # check if it delete successfully\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_hash</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>obtained_date</th>\n",
       "      <th>valence</th>\n",
       "      <th>music_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>The Beatles</td>\n",
       "      <td>7hpFYWL3cw5m4y70cce7Zb</td>\n",
       "      <td>Day Tripper - Remastered 2015</td>\n",
       "      <td>68</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>0.665</td>\n",
       "      <td>169000.0</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>F#</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-8.438</td>\n",
       "      <td>Minor</td>\n",
       "      <td>0.0307</td>\n",
       "      <td>137.453</td>\n",
       "      <td>4-Apr</td>\n",
       "      <td>0.731</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The Beatles</td>\n",
       "      <td>7JRN5xOUIrnI4crUMOt6X4</td>\n",
       "      <td>I Feel Fine - Remastered 2015</td>\n",
       "      <td>63</td>\n",
       "      <td>0.0904</td>\n",
       "      <td>0.564</td>\n",
       "      <td>139347.0</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>G</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-7.089</td>\n",
       "      <td>Major</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>89.847</td>\n",
       "      <td>4-Apr</td>\n",
       "      <td>0.912</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>Sugarland</td>\n",
       "      <td>507bYMYfbm6sUS9iEAaeSd</td>\n",
       "      <td>Something More</td>\n",
       "      <td>53</td>\n",
       "      <td>0.3130</td>\n",
       "      <td>0.590</td>\n",
       "      <td>216733.0</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>E</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-4.419</td>\n",
       "      <td>Major</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>102.265</td>\n",
       "      <td>4-Apr</td>\n",
       "      <td>0.415</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Vinyl Theatre</td>\n",
       "      <td>7IQsZqlZ53UIXFjzHOkraF</td>\n",
       "      <td>Breaking Up My Bones</td>\n",
       "      <td>61</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.608</td>\n",
       "      <td>185429.0</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>D</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-3.185</td>\n",
       "      <td>Major</td>\n",
       "      <td>0.0392</td>\n",
       "      <td>105.032</td>\n",
       "      <td>4-Apr</td>\n",
       "      <td>0.652</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>MKTO</td>\n",
       "      <td>7K8KNuwZAKKktkfPMosFsM</td>\n",
       "      <td>American Dream</td>\n",
       "      <td>64</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.554</td>\n",
       "      <td>225747.0</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>C</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-5.043</td>\n",
       "      <td>Major</td>\n",
       "      <td>0.0497</td>\n",
       "      <td>122.846</td>\n",
       "      <td>4-Apr</td>\n",
       "      <td>0.497</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         artist_name              track_hash                     track_name  \\\n",
       "49995    The Beatles  7hpFYWL3cw5m4y70cce7Zb  Day Tripper - Remastered 2015   \n",
       "49996    The Beatles  7JRN5xOUIrnI4crUMOt6X4  I Feel Fine - Remastered 2015   \n",
       "49997      Sugarland  507bYMYfbm6sUS9iEAaeSd                 Something More   \n",
       "49998  Vinyl Theatre  7IQsZqlZ53UIXFjzHOkraF           Breaking Up My Bones   \n",
       "49999           MKTO  7K8KNuwZAKKktkfPMosFsM                 American Dream   \n",
       "\n",
       "       popularity  acousticness  danceability  duration_ms  energy  \\\n",
       "49995          68        0.1200         0.665     169000.0   0.782   \n",
       "49996          63        0.0904         0.564     139347.0   0.827   \n",
       "49997          53        0.3130         0.590     216733.0   0.850   \n",
       "49998          61        0.0125         0.608     185429.0   0.899   \n",
       "49999          64        0.1140         0.554     225747.0   0.767   \n",
       "\n",
       "       instrumentalness key  liveness  loudness   mode  speechiness    tempo  \\\n",
       "49995          0.000004  F#     0.125    -8.438  Minor       0.0307  137.453   \n",
       "49996          0.000004   G     0.127    -7.089  Major       0.0283   89.847   \n",
       "49997          0.000000   E     0.116    -4.419  Major       0.0582  102.265   \n",
       "49998          0.000000   D     0.211    -3.185  Major       0.0392  105.032   \n",
       "49999          0.000000   C     0.231    -5.043  Major       0.0497  122.846   \n",
       "\n",
       "      obtained_date  valence music_genre  \n",
       "49995         4-Apr    0.731        Rock  \n",
       "49996         4-Apr    0.912        Rock  \n",
       "49997         4-Apr    0.415        Rock  \n",
       "49998         4-Apr    0.652        Rock  \n",
       "49999         4-Apr    0.497        Rock  "
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## only delete NA for `artist_name` feature since it is MCAR and it is under 5%\r\n",
    "\r\n",
    "This stackoverflow contains everything about different ways of delete NA:\r\n",
    "> https://stackoverflow.com/questions/13413590/how-to-drop-rows-of-pandas-dataframe-whose-value-in-a-certain-column-is-nan"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "\r\n",
    "# TODO: FIXME: It can be upgraded in the future! Maybe in completion or challenge part!\r\n",
    "# For this one, it may use feature transformation\r\n",
    "\r\n",
    "print(\"Shape: Before delete artist_name NA:\",merged_dataset.shape)\r\n",
    "whole_noArtistName = merged_dataset.dropna(subset = ['artist_name'], inplace=False)\r\n",
    "print(\"Shape: After delete artist_name NA:\",whole_noArtistName.shape)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape: Before delete artist_name NA: (50000, 18)\n",
      "Shape: After delete artist_name NA: (47511, 18)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fill the rest NA by mean value.\r\n",
    "\r\n",
    "TODO: FIXME: It can be upgraded in the future! Maybe in completion or challenge part!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# TODO: FIXME: It can be upgraded in the future! Maybe in completion or challenge part!\r\n",
    "whole_noMissing = whole_noArtistName.copy()\r\n",
    "\r\n",
    "\r\n",
    "print('before:')\r\n",
    "print(myutil.print_NA_details(whole_noMissing))\r\n",
    "print(whole_noMissing.shape)\r\n",
    "print('-'*80)\r\n",
    "\r\n",
    "# whole_noMissing['tempo'].fillna((whole_noMissing['tempo'].mean()), inplace=True)\r\n",
    "# whole_noMissing['duration_ms'].fillna((whole_noMissing['duration_ms'].mean()), inplace=True)\r\n",
    "whole_noMissing =  myutil.replace_by_mean(whole_noMissing)\r\n",
    "\r\n",
    "print('-'*80)\r\n",
    "print('After:')\r\n",
    "print(myutil.print_NA_details(whole_noMissing))\r\n",
    "print(whole_noMissing.shape)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "before:\n",
      "\n",
      "missing feature and values:\n",
      " duration_ms    4696\n",
      "tempo          4720\n",
      "dtype: int64\n",
      "None\n",
      "(47511, 18)\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "After:\n",
      "\n",
      "missing feature and values:\n",
      " Series([], dtype: int64)\n",
      "None\n",
      "(47511, 18)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\r\n",
    "whole_noMissing.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 47511 entries, 0 to 49999\n",
      "Data columns (total 18 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   artist_name       47511 non-null  object \n",
      " 1   track_hash        47511 non-null  object \n",
      " 2   track_name        47511 non-null  object \n",
      " 3   popularity        47511 non-null  int64  \n",
      " 4   acousticness      47511 non-null  float64\n",
      " 5   danceability      47511 non-null  float64\n",
      " 6   duration_ms       47511 non-null  float64\n",
      " 7   energy            47511 non-null  float64\n",
      " 8   instrumentalness  47511 non-null  float64\n",
      " 9   key               47511 non-null  object \n",
      " 10  liveness          47511 non-null  float64\n",
      " 11  loudness          47511 non-null  float64\n",
      " 12  mode              47511 non-null  object \n",
      " 13  speechiness       47511 non-null  float64\n",
      " 14  tempo             47511 non-null  float64\n",
      " 15  obtained_date     47511 non-null  object \n",
      " 16  valence           47511 non-null  float64\n",
      " 17  music_genre       47511 non-null  object \n",
      "dtypes: float64(10), int64(1), object(7)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Below are From Week7 tutorial. TODO:\r\n",
    "\r\n",
    "By observating the pp report, and output above, i observe that `tempo` should be the numerical feature, but it report that it's categorical feature. This is caused by the value `?` is categorical, and it occur 4980 time, which is less than 10%.\r\n",
    "\r\n",
    "By googling, I think `tempo` a very important feature, from this [tutorial](https://towardsdatascience.com/all-about-missing-data-handling-b94b8b5d2184) and school slide, I find that although it is MCAR, but the missing percentage is greater tahn 5%, which means, just delete it is not an approiate approach, the `imputation` is better approach, so  I decide to try to apply it based on the school tutorial(week7_tut_FCDI.ipynb) using :  \r\n",
    "> panda interpolation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# # Now we will split the data to train and test subsets as **ONLY** the training data will be used to learn the imputers then the learnt models are applied to the test data\r\n",
    "\r\n",
    "# X_whole=merged_dataset[merged_dataset.columns[1:-1]]\r\n",
    "# y_whole=merged_dataset[merged_dataset.columns[-1]]\r\n",
    "\r\n",
    "# X_whole_train, X_whole_test, y_whole_train, y_whole_test = train_test_split(X_whole, y_whole, test_size=0.3, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Encoding categorical features with preserving the missing values in incomplete features\r\n",
    "```python\r\n",
    "from sklearn.preprocessing import OrdinalEncoder\r\n",
    "```\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# encoder = OrdinalEncoder(\r\n",
    "#     handle_unknown='use_encoded_value', unknown_value=np.nan)\r\n",
    "# X_whole_train_encoded = X_whole_train.copy()\r\n",
    "\r\n",
    "\r\n",
    "# X_titanic_train_encoded['sex'] = encoder_sex.fit_transform(\r\n",
    "#     X_titanic_train_encoded['sex'].values.reshape(-1, 1))\r\n",
    "\r\n",
    "# # Now lets encode the incomplete Cabin feature\r\n",
    "# # You can use the same encoder for both but we use two for the sake of clarfication\r\n",
    "# encoder_cabin = OrdinalEncoder(\r\n",
    "#     handle_unknown='use_encoded_value', unknown_value=np.nan)\r\n",
    "# X_titanic_train_encoded['cabin'] = encoder_cabin.fit_transform(\r\n",
    "#     X_titanic_train_encoded['cabin'].values.reshape(-1, 1).astype(str))\r\n",
    "# # get the code of the \"nan\" value for the cabin categorical feature\r\n",
    "# cabin_nan_code = encoder_cabin.transform([['nan']])[0][0]\r\n",
    "# # print(cabin_nan_code)\r\n",
    "# # Now, retrive the nan values to be missing in the encoded data\r\n",
    "# X_titanic_train_encoded['cabin'].replace(cabin_nan_code, np.nan, inplace=True)\r\n",
    "\r\n",
    "# # X_titanic_train_encoded is the encoded incomplete training data\r\n",
    "# #Check the types of the encoded data, no object features\r\n",
    "# X_titanic_train_encoded.info()\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# X_whole_data_complete=X_whole_train_encoded.copy()\r\n",
    "# X_whole_data_complete=X_whole_data_complete.interpolate()\r\n",
    "# #The output is 'numpy.ndarray' so we convert it to dataframe for consistency\r\n",
    "# X_whole_train_complete=pd.DataFrame(X_whole_train_complete)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Encoding Categorical data \r\n",
    "\r\n",
    "Encoding Categorical Variables. This step is for converting categorical data to numerical data. We have 2 categorical variables which are country and regional Indicator, so after this step, they will be translate to numerical data in order forpreparing to the machine learning. I use LabelEncoder() method from tutorial to encode them into the numerical valuesand replace it to the dataset\r\n",
    "\r\n",
    "> https://zhuanlan.zhihu.com/p/117230627"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "categorical = {'artist_name':0, 'track_hash':0,'track_name':0,'key':0,'obtained_date':0,'mode':0}#,'music_genre':0}\r\n",
    "\r\n",
    "whole_encode_all = whole_noMissing.copy()\r\n",
    "\r\n",
    "# 1st, use label encoder to encode mode and the class label: music_genre\r\n",
    "whole_encode_all = myutil.encode_class_label(whole_encode_all)\r\n",
    "whole_encode_all = myutil.encode_feature_mode(whole_encode_all)\r\n",
    "\r\n",
    "# 2nd, use ordinal encoder to encode 'key','obtained_date' these 2 features\r\n",
    "whole_encode_all = myutil.ordinal_encoder(whole_encode_all)\r\n",
    "\r\n",
    "# 3rd, use one hot encoder to encode 'artist_name','track_hash','track_name'\r\n",
    "# whole_encode_all = myutil.one_hot_encoder(whole_encode_all)\r\n",
    "feature_list = ['artist_name','track_hash','track_name']\r\n",
    "whole_encode_all = myutil.one_hot_encoder(whole_encode_all,feature_list=feature_list)\r\n",
    "\r\n",
    "# encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\r\n",
    "# for i in range(len(feature_list)):\r\n",
    "#     whole_encode_all[feature_list[i]] = encoder.fit_transform(whole_encode_all[feature_list[i]].to_numpy().reshape(-1, 1))\r\n",
    "    \r\n",
    "# whole_encode_all[feature_list[0]] = encoder.fit_transform(whole_encode_all[feature_list[0]].to_numpy().reshape(-1, 1))\r\n",
    "whole_encode_all.info()\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 14.1 GiB for an array with shape (47511, 39917) and data type float64",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6f62d860cb59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# whole_encode_all = myutil.one_hot_encoder(whole_encode_all)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mfeature_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'artist_name'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'track_hash'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'track_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mwhole_encode_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhole_encode_all\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\11973\\Desktop\\309Ass\\COMP309a3\\myutil.py\u001b[0m in \u001b[0;36mone_hot_encoder\u001b[1;34m(dataset, feature_list)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m         dataset[feature_list[i]] = encoder.fit_transform(\n\u001b[0m\u001b[0;32m    114\u001b[0m             dataset[feature_list[i]].to_numpy().reshape(-1, 1))\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3161\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3162\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3163\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3240\u001b[0m         \"\"\"\n\u001b[0;32m   3241\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3242\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3243\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, key, value, broadcast)\u001b[0m\n\u001b[0;32m   3904\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3905\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3906\u001b[1;33m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3907\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3908\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 14.1 GiB for an array with shape (47511, 39917) and data type float64"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Normalisation. \r\n",
    "\r\n",
    "This step is to transform columns to a consistent set of rules, which means need to meet the convention.Due to the scales of each feature  is different and not unified, and some machine learning methods are greatly affectedby it, so this step is fundamental and essential. I normalize all the feature columns except for the ladder score to theconsistent set of scales"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Discretisation.\r\n",
    "\r\n",
    " ~~Since the happiness ladder score is the target and it is discreate numerical, so in order to do the machinelearning, this attribute need to be converted to the nominal attribute. I apply the discretisation to transform it into 4different happiness ladder levels, so that the machine is able to learn~~"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}